<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: velocityconf | Charles' Blog]]></title>
  <link href="http://ccare.github.com/blog/categories/velocityconf/atom.xml" rel="self"/>
  <link href="http://ccare.github.com/"/>
  <updated>2013-10-23T21:52:25+01:00</updated>
  <id>http://ccare.github.com/</id>
  <author>
    <name><![CDATA[Charles Care]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Openstack and Cloudera workshops at Velocity]]></title>
    <link href="http://ccare.github.com/blog/2011/06/28/openstack-and-cloudera-workshops-at-velocity/"/>
    <updated>2011-06-28T21:47:00+01:00</updated>
    <id>http://ccare.github.com/blog/2011/06/28/openstack-and-cloudera-workshops-at-velocity</id>
    <content type="html"><![CDATA[<p>The first workshop I attended at Velocity was run by Rib Pedde, Todd Willey, and Matt Ray, and discussed the Openstack project. This was less of a workshop, and more of a talk in three parts. Interesting, none-the-less. Openstack is a virtualisation platform which has gained quite a lot of momentum in the last year.</p>

<p><a href="http://velocityconf.com/velocity2011/public/schedule/detail/19970">http://velocityconf.com/velocity2011/public/schedule/detail/19970</a></p>

<p>The majority of the session provided an overview of the different components of OpenStack as they stand today and a look at the project&rsquo;s road map. It seems that a lot has happened to this project over the last couple of months and they seem to have quite an interesting set of features coming in the next couple of months.</p>

<!-- more -->


<p>As the speakers presented it, there are two core components Openstack: OpenStack Compute (or Nova), and OpenStack Object Store, (or Swift). In Amazon speak, these are the OpenStack equivalents of EC2 and S3. Both the virtualisation and storage layers aim to be scalable, back-end agnostic, and use eventually consistent.</p>

<p>The speakers went through the architecture of a what a compute cluster would look like. Compute nodes run the virtual machines and are typically commodity servers with a public, private, and management network. After the talk, some people were asking whether all three interfaces were necessary. The speakers said that they weren&rsquo;t, but they did recommended separating management/private traffic from the public traffic.</p>

<p>One of the interesting features was to see they&rsquo;ve got an EC2 compatible which is nice. Another thing that was interesting is that while they store metadata in SqlAlchemy for fast access, the canonical source of the cluster&rsquo;s state is derived from the state of the virtualisation layer. Not sure exactly how they&rsquo;re deriving firewall and networking configuration &ndash; that&rsquo;s one for me to read more about.</p>

<p>Another feature I liked was the concept Utility VM &ndash; a kind of virtual equivalent of a hardware appliance. A Utility VM essentially allows them to do various virtualisation offerings such as Database as service.</p>

<p>They also discussed their API, which unsurprisingly makes use of async rest calls. Someone asked if they had plans to provide a synchronous api, the response was that clients should get feedback by polling. However, the speakers indicated that there might be support for a pub-sub mechanism with callbacks at some point in the future. I found the fact they were considering callbacks interesting. Although it would add state to their infrastructure, it&rsquo;s clearly a feature that clients want.</p>

<p>Interesting upcoming features:</p>

<ul>
<li>Quantum: Networking as a service (with Cisco)</li>
<li>Burrow: HTTP message queue</li>
<li>Red Dwarf: Database as a service</li>
<li>Keystone &ndash; pluggable auth for all OpenStack components</li>
<li>lunr &ndash; volumes as service</li>
<li>A dashboard to manage the stack</li>
<li>Atlas: load balancing as a service (backed by Zeus)</li>
</ul>


<p>All of these components are clearly being designed with a specific initial vendor in mind (e.g. Zeus for their load-balancing solution) but the general OpenStack philosophy seems to be to make these back-ends pluggable and introduce additional back-ends later on.</p>

<p>They then talked a little around the design of Swift, and how enterprise storage was expensive and requirements were going to double every year. Swift needed to be designed for failure with self healing using commodity hardware. To provide resilience, Swift has the concept of an &lsquo;availability zone&rsquo; inside a cluster&hellip; it&rsquo;s an overloaded term, but an OpenStack avaliablity zone they mean a set of storage resources that might all die together in the event of a network, cooling or power failure. Data is then stored in 3 different zones.</p>

<p>A other few parts of the architecture I found interesting:</p>

<ul>
<li>To manage where the data is stored they have a coordination function named the Ring. Interesting story of how the Ring&rsquo;s implementation evolved. They started with a DB and although they knew it wasn&rsquo;t going to scale long term it quickly became unmanageble. It was an interesting lesson-learnt that they &ldquo;didn&rsquo;t realise how amazingly quickly [a relational db] becomes a bottleneck&rdquo;. One member of the audience asked if the ring was the bottleneck&hellip; the speakers thought that networking would be the true bottleneck.</li>
<li>To achieve better scalability they distributed the work by pushing work into the replicas. They defer checking and auditing etc to the replicas rather than in a central coordination, and they now store metadata in sqllite databases within Swift.</li>
<li>Objects are versioned using system time, so at Rackspace everything&rsquo;s synchronised over NTP. For the distributed databases (sqllite stored in Swift), the NTP timestamps are used for row-based synchronisation</li>
</ul>


<p>Finally, Matt Ray talked a little about using Chef to manage OpenStack. He gave an intro to Chef and the idea of infrastructure as code.</p>

<p>Knife openstack is a project to bring the equivalent management utilities to ec-tools.
The other interesting tool he mentioned was Crowbar from Dell which is a PXE-based tool to bootstrap openstack and chef.
One interesting point he made was that OpenStack was evolving so fast that he was having to create a separate Chef cookbook for each major version. I found that interesting because we&rsquo;ve seen a similar issue with needing to manage and version several parallel branches of Puppet config.</p>

<p>The final workshop I attended at Velocity was</p>

<p>&ldquo;Managing the System Lifecycle and Configuration of Apache Hadoop and Other Distributed Systems&rdquo;
/ Philip Zeyliger Cloudera</p>

<p>The talk introduced Hadoop and it&rsquo;s various components, which was handy as there always seem to be so many.</p>

<ul>
<li>HDFS</li>
<li>MapReduce</li>
<li>ZooKeeper</li>
<li>HBase</li>
<li>Oozie</li>
<li>HUE</li>
</ul>


<p>He discussed how different people were managing Hadoop hosts (PXE, kickstart, Cobbler) and software (packaging, file transfer, installer scripts).</p>

<p>An interesting comment made during the talk were that we still talk about distributed systems are clusters of nodes whereas Hadoop is all about programming a datacentre as if it were one node. So, for instance, puppet and chef are designed to manage N hosts running a number of services&hellip; but with hadoop you&rsquo;re trying to manage a N hosts running ONE distributed/failsafe service.</p>

<p>He introduced the Cloudera Service and Configuration Manager (SCM) which is essentially a central console with log and status aggregation and the Cloudera Activity Monitor, which he described as being like UNIX top for hadoop.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NodeJS at Velocity]]></title>
    <link href="http://ccare.github.com/blog/2011/06/26/nodejs-at-velocity/"/>
    <updated>2011-06-26T21:17:00+01:00</updated>
    <id>http://ccare.github.com/blog/2011/06/26/nodejs-at-velocity</id>
    <content type="html"><![CDATA[<p>NodeJS seemed to be a passing theme in a number of the talks at the conference,
including:</p>

<ul>
<li>&ldquo;Performance Enhancing Programming with NodeJS&rdquo; by Tim Caswell</li>
<li>Bryan Cantrill&rsquo;s &ldquo;Instrumenting the real-time web: Node.js, DTrace and the Robinson Projection&rdquo;</li>
<li>&ldquo;Cast &ndash; The Open Deployment Platform&rdquo; by Paul Querna</li>
<li>&ldquo;Real-Time, Real-Fast&rdquo; by Dylan Schiemann</li>
</ul>


<p>I attended Tim Caswell&rsquo;s and Bryan Cantrill&rsquo;s sessions, both of which were great
introductions to Node.js hipe</p>

<p>Tim Caswell&rsquo;s (creationix on twitter/github) workshop was a great demo of
Node&rsquo;s scalability. He spun up a webserver on a private wlan and got the whole
room to start pinging various demo apps.</p>

<p>A few things discussed:</p>

<p>Running JavaScript on client and server can avoid the issue of doubling your
effort&hellip; You don&rsquo;t cross the boundary all the time if pieces of modular
functionality can be run either client side or server side.
There&rsquo;s an opportunity to run more in the server during the period that the
Browser&rsquo;s busy downloading content or engaged in pre-render tasks. Once the
app is fully up, you can then progressively do more in the client by shipping
the same scripts over the wire.
Non blocking IO. Using buffers and memcpy to avoid string builder overhead
Some discussion on tooling: node-inspector, DTrace, node -debug, and v8
graphical debug all sound interesting.</p>

<p>Bryan Cantrill&rsquo;s talk was, in a single word, energetic. Unsurprisingly we saw a
lot of DTrace and we heard about a Node.js app he&rsquo;d build to monitor net traffic
during a hackday. I think that Cantrill&rsquo;s talk can be summarised as Node being
the union of the popularity of JavaScript, non-blocking IO, and the Unix APIs
(or, as Cantrill puts it &ldquo;the APIs God intended&rdquo;).</p>

<p>In summary, there seems to be a lot of hype around Node. And it seems that
it&rsquo;s the mix of JavaScript, Closures, and Non-blocking IO that are capturing
the mood. Also because JavaScript never had any IO in the core language, you
don&rsquo;t have to drop the bits of the language that don&rsquo;t fit the Node.js model
(e.g. if you&rsquo;re using twisted you&rsquo;re only wanting to use a subset of python).
I&rsquo;ll definitely be playing with it to understand more.</p>

<p>Of course, the backbone of Node is the V8 JavaScript engine, and the theme of
JavaScript tuning was definitely strong at Velocity. Another interesting keynote
was Douglas Crockford on JavaScript. He was talking about the JavaScript
benchmarks and how a lack of good benchmarks had resulted in the wrong kind of
optimisations. Recently Crockford created a benchmark by running jslint over
jslint. He argued that this provided a much better benchmark as it touched more
widely-used patterns (such as closures and prototypal inheritance) than the more
common &ldquo;how fast does this loop execute&rdquo; tests often found in benchmarks. He
also shared some interesting stats on performance gains that the V8 team had
been able to make using his benchmark.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Statistics @ Velocity]]></title>
    <link href="http://ccare.github.com/blog/2011/06/11/statistics-at-velocity/"/>
    <updated>2011-06-11T21:12:00+01:00</updated>
    <id>http://ccare.github.com/blog/2011/06/11/statistics-at-velocity</id>
    <content type="html"><![CDATA[<p>One of the pre-conference workshops was an excellent session by John Rauser from
Amazon. I&rsquo;ve never been hugely confident at my abilities to do estimation,
forecasting, capacity planning etc. It&rsquo;s a really difficult thing.</p>

<p>I&rsquo;d heard that Rauser was an excellent speaker, so I attended his workshop on
statistics. For anyone interested, Rauser also did a shorter keynote later in
the conference, which you can catch on YouTube. A big theme from these
discussions was understanding the reduction that your statistical reporting
makes. Averages, variances, etc are all summary statistics &ndash; but how often to we
really think of them as lossy formats?</p>

<p>  My main takeaways from both of his talk were to try and understand the
  lossiness of summary stats. I particularly liked his summary statistics of
  Moby Dick. Another major theme in his talks was to make time to do deep dives
  into log data, even if it only means you know how to when something really
  goes wrong.</p>

<!-- more -->


<p>  During the workshop, John discussed how we approach estimating someone&rsquo;s age.
  Given that problem, the instinct reaction is to propose a estimated value: 45,
  58, 60 etc. However, to quote Rauser: we &lsquo;[shouldn&rsquo;t] give a single number
  unless you&rsquo;re forced to. We should always give a range, ideally with an agreed
  confidence interval such as 90%. I can guess someone&rsquo;s age with 100%
  confidence if I state their age is between 0 and 150, but that&rsquo;s not very
  useful. But by combining additional information and reducing absolute
  certainty I can start to narrow that down. This isn&rsquo;t rocket science, but how
  many times have we been asked &ldquo;when will this be ready&rdquo;, or &ldquo;when&rsquo;s the
  delivery date&rdquo;? And in the face of those questions, how often have we given a
  range with a stated confidence. Perhaps we feel that our project managers
  would laugh us out of the room? Project managers like fixed dates.</p>

<p>However, in complex projects &ndash; and IT projects are usually complex &ndash; a range is
often more honest, and would often better express the reality of the situation.
Again, this is nothing new &ndash; if you have an Agile Coach on your team, they will
probably recommend estimating using ranges or 3-point estimates. However, it&rsquo;s
easy to slip into the default behaviour of giving a single numeric response.</p>

<p>  Throughout the talk, Rauser covered other examples, such as estimating the
  depth of his desk, and discussed how measurement improves our ability to
  estimate with confidence.</p>

<p>  As the workshop continued, Rauser set us an estimating task. He asked us to
  provide estimates (ranges, of course!) for a number of questions. The key was
  not the specific answers, but rather that we were aiming for 90% confidence
  estimates&hellip; i.e we should have got 18 out of the 20 questions correct. We
  then graphed the results of a portion of the room and found that most people
  averaged between 20% and 40% accuracy. An interesting demo of how humans seem
  to default to be over-optimistic in their estimating.</p>

<p>  In the final section of the workshop, Rauser discussed the nature of
  statistics &ndash; how school curriculum are still teaching stats the &lsquo;old way&rsquo; and
  how for practical analysis, it&rsquo;s really much better to tackle stats correctly
  using computer modelling. Do we need to learn about how to apply statistical
  tests and derive summary statistics? After all, summary stats are lossy and
  stop you looking at your data? Are they more dangerous in the hands of the
  layman that just getting a computer to process the raw stats? We saw a demo of
  some of the statistical features of excel, and then, a demo of more powerful
  tools like <strong>R</strong> and <strong>Sage</strong>.</p>

<p>  In conclusion: Rauser left us with a few final comments:</p>

<ul>
<li>everything is uncertain</li>
<li>even counting breaks down at scale</li>
<li>estimates need ranges</li>
<li>single numbers are decisions</li>
</ul>


<p>A great session.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Velocity 2011... a great conference]]></title>
    <link href="http://ccare.github.com/blog/2011/06/11/velocity-2011-dot-dot-dot-a-great-conference/"/>
    <updated>2011-06-11T20:27:00+01:00</updated>
    <id>http://ccare.github.com/blog/2011/06/11/velocity-2011-dot-dot-dot-a-great-conference</id>
    <content type="html"><![CDATA[<p>It&rsquo;s a week now since I came back from
<a href="http://velocityconf.com/velocity2011">O'Reilly&rsquo;s Velocity</a>
conference in San
Jose so I&rsquo;ve had a chance to let some of the thoughts settle in my mind. I
attended both the main conference and the workshops the day before.</p>

<p>Attending a large tech conference is a great way of getting up to speed with the
latest technical themes and meet some great people. Velocity&rsquo;s three days long,
but tacked on the end was DevopsDays &ndash; a more informal meet-up of people
interested in bringing the traditionally separate roles of development and
operations together. Devops was also an important theme at Velocity. Or, as Theo
Schlossnagle said in
his <a href="http://www.youtube.com/watch?v=y0mHo7SMCQk&amp;feature=player_embedded">Velocity keynote on careers</a>&hellip;
it&rsquo;s really about *ops.
Whatever technology you build, that software&rsquo;s delivering a service, and
whatever that service, it must operate. Or to put it from another way Theo was
saying that we should treat &lsquo;operability like security&rsquo;.</p>

<p>For the last couple of months, I&rsquo;ve been working within the platform team at
<a href="http://www.talis.com">Talis</a>. We&rsquo;re a small team, but have a lot we want to build and manage. Trying to
organise ourselves in a devops is something really important to the way we work.
Personally, I&rsquo;ve never felt comfortable with the traditional &lsquo;throw it over the
wall&rsquo; relationship between software and operations teams. I&rsquo;ve worked in a
number of different kinds of teams before, usually I&rsquo;ve tried to be a developer
who works closely with ops, but I&rsquo;ve definitely fallen into the us and them trap
from time to time. Like agile, devops is not a magic formula, but I definitely
feel much more comfortable with everyone being part of the same team.</p>

<p>I&rsquo;ve decided to blog some of my thoughts/notes from the conference: the posts
should start to appear very soon.</p>
]]></content>
  </entry>
  
</feed>
